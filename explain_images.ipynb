{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Explainability with Captum - Tutorial\n",
        "\n",
        "This notebook demonstrates how to use the modular `image.py` functions for explainability analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 1: Using the High-Level Function\n",
        "\n",
        "The easiest way - analyze all images with all methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXAMPLE 1: High-Level Function Usage\n",
            "================================================================================\n",
            "\n",
            "âœ“ Found images in: ./data/fracture_samples\n",
            "\n",
            "Analyzing images in ./data/fracture_samples with all methods...\n",
            "(Processing up to 2 images for demo)\n",
            "\n",
            "Selected methods: gradientshap, integratedgradients, saliency\n",
            "Using device: cpu\n",
            "Loading model: microsoft/resnet-50\n",
            "Directory detected: ./data/fracture_samples\n",
            "\n",
            "Found 2 images to analyze\n",
            "\n",
            "================================================================================\n",
            "EXPLAINABILITY ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "--- Image 1/2: brick_texture.png ---\n",
            "Directory detected: ./data/fracture_samples\n",
            "\n",
            "Found 2 images to analyze\n",
            "\n",
            "================================================================================\n",
            "EXPLAINABILITY ANALYSIS RESULTS\n",
            "================================================================================\n",
            "\n",
            "--- Image 1/2: brick_texture.png ---\n",
            "Predicted class: 828 (confidence: 0.4866)\n",
            "Computing GradientShap attributions...\n",
            "Predicted class: 828 (confidence: 0.4866)\n",
            "Computing GradientShap attributions...\n",
            "GradientShap attribution range: [0.0001, 0.0962]\n",
            "GradientShap mean absolute attribution: 0.0059\n",
            "Computing Integrated Gradients attributions...\n",
            "GradientShap attribution range: [0.0001, 0.0962]\n",
            "GradientShap mean absolute attribution: 0.0059\n",
            "Computing Integrated Gradients attributions...\n",
            "IntegratedGradients attribution range: [0.0001, 0.2719]\n",
            "IntegratedGradients mean absolute attribution: 0.0155\n",
            "Computing Saliency attributions...\n",
            "Saliency attribution range: [0.0007, 1.1264]\n",
            "Saliency mean absolute attribution: 0.0588\n",
            "IntegratedGradients attribution range: [0.0001, 0.2719]\n",
            "IntegratedGradients mean absolute attribution: 0.0155\n",
            "Computing Saliency attributions...\n",
            "Saliency attribution range: [0.0007, 1.1264]\n",
            "Saliency mean absolute attribution: 0.0588\n",
            "    Saved: ./outputs/images/brick_texture_gradientshap.png\n",
            "    Saved: ./outputs/images/brick_texture_integrated_gradients.png\n",
            "    Saved: ./outputs/images/brick_texture_gradientshap.png\n",
            "    Saved: ./outputs/images/brick_texture_integrated_gradients.png\n",
            "    Saved: ./outputs/images/brick_texture_saliency.png\n",
            "\n",
            "Creating combined comparison visualization...\n",
            "    Saved: ./outputs/images/brick_texture_saliency.png\n",
            "\n",
            "Creating combined comparison visualization...\n",
            "Saved: ./outputs/images/brick_texture_comparison.png\n",
            "\n",
            "--- Image 2/2: coins_metal.png ---\n",
            "Predicted class: 677 (confidence: 0.9874)\n",
            "Computing GradientShap attributions...\n",
            "Saved: ./outputs/images/brick_texture_comparison.png\n",
            "\n",
            "--- Image 2/2: coins_metal.png ---\n",
            "Predicted class: 677 (confidence: 0.9874)\n",
            "Computing GradientShap attributions...\n",
            "GradientShap attribution range: [0.0001, 0.1872]\n",
            "GradientShap mean absolute attribution: 0.0164\n",
            "Computing Integrated Gradients attributions...\n",
            "GradientShap attribution range: [0.0001, 0.1872]\n",
            "GradientShap mean absolute attribution: 0.0164\n",
            "Computing Integrated Gradients attributions...\n",
            "IntegratedGradients attribution range: [0.0002, 0.7689]\n",
            "IntegratedGradients mean absolute attribution: 0.0440\n",
            "Computing Saliency attributions...\n",
            "Saliency attribution range: [0.0007, 1.0313]\n",
            "Saliency mean absolute attribution: 0.0565\n",
            "    Saved: ./outputs/images/coins_metal_gradientshap.png\n",
            "IntegratedGradients attribution range: [0.0002, 0.7689]\n",
            "IntegratedGradients mean absolute attribution: 0.0440\n",
            "Computing Saliency attributions...\n",
            "Saliency attribution range: [0.0007, 1.0313]\n",
            "Saliency mean absolute attribution: 0.0565\n",
            "    Saved: ./outputs/images/coins_metal_gradientshap.png\n",
            "    Saved: ./outputs/images/coins_metal_integrated_gradients.png\n",
            "    Saved: ./outputs/images/coins_metal_saliency.png\n",
            "\n",
            "Creating combined comparison visualization...\n",
            "    Saved: ./outputs/images/coins_metal_integrated_gradients.png\n",
            "    Saved: ./outputs/images/coins_metal_saliency.png\n",
            "\n",
            "Creating combined comparison visualization...\n",
            "Saved: ./outputs/images/coins_metal_comparison.png\n",
            "\n",
            "================================================================================\n",
            "SUMMARY\n",
            "================================================================================\n",
            "âœ“ Analyzed 2 images.\n",
            "âœ“ Applied 3 explainability method(s): gradientshap, integratedgradients, saliency.\n",
            "âœ“ Generated visualizations saved to: ./outputs/images.\n",
            "\n",
            "Key insights:\n",
            "- GradientShap: Captures feature importance using gradient-based Shapley values.\n",
            "- Integrated Gradients: Shows pixel-level attribution by integrating gradients along path.\n",
            "- Saliency: Highlights regions with highest gradient magnitude.\n",
            "\n",
            "All results saved in: ./outputs/images\n",
            "\n",
            "âœ“ Analysis complete! Check ./outputs/images/ for results\n",
            "Saved: ./outputs/images/coins_metal_comparison.png\n",
            "\n",
            "================================================================================\n",
            "SUMMARY\n",
            "================================================================================\n",
            "âœ“ Analyzed 2 images.\n",
            "âœ“ Applied 3 explainability method(s): gradientshap, integratedgradients, saliency.\n",
            "âœ“ Generated visualizations saved to: ./outputs/images.\n",
            "\n",
            "Key insights:\n",
            "- GradientShap: Captures feature importance using gradient-based Shapley values.\n",
            "- Integrated Gradients: Shows pixel-level attribution by integrating gradients along path.\n",
            "- Saliency: Highlights regions with highest gradient magnitude.\n",
            "\n",
            "All results saved in: ./outputs/images\n",
            "\n",
            "âœ“ Analysis complete! Check ./outputs/images/ for results\n"
          ]
        }
      ],
      "source": [
        "# Example 1: Using the High-Level Function\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, './src')\n",
        "\n",
        "from images import explain_model_predictions\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 1: High-Level Function Usage\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check if images exist in common directories\n",
        "possible_paths = ['./data/fracture_samples', './data/generic_samples', './data/sample_images', './images/', './samples/']\n",
        "data_path = None\n",
        "\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path) and os.path.isdir(path):\n",
        "        # Check if directory has images\n",
        "        image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')\n",
        "        images = [f for f in os.listdir(path) if f.lower().endswith(image_extensions)]\n",
        "        if images:\n",
        "            data_path = path\n",
        "            print(f\"\\nâœ“ Found images in: {path}\")\n",
        "            break\n",
        "\n",
        "if data_path:\n",
        "    print(f\"\\nAnalyzing images in {data_path} with all methods...\")\n",
        "    print(\"(Processing up to 2 images for demo)\\n\")\n",
        "    \n",
        "    # Run the complete analysis\n",
        "    explain_model_predictions(\n",
        "        model_path='microsoft/resnet-50',\n",
        "        data_path=data_path,\n",
        "        output_dir='./outputs/images',\n",
        "        num_samples=2,               # Limit to 2 images for demo\n",
        "        methods=None                 # Use ALL methods\n",
        "    )\n",
        "    \n",
        "    print(\"\\nâœ“ Analysis complete! Check ./outputs/images/ for results\")\n",
        "else:\n",
        "    print(\"\\nâ„¹ No images found in \", possible_paths)\n",
        "    print(\"\\n USAGE EXAMPLE (add images to run):\")\n",
        "    print(\"\"\"\n",
        "explain_model_predictions(\n",
        "    model_path='microsoft/resnet-50',\n",
        "    data_path='./my_images',        # Directory with images\n",
        "    output_dir='./outputs/images',\n",
        "    num_samples=None,               # Process ALL images (default)\n",
        "    methods=None                    # Use ALL methods (default)\n",
        ")\n",
        "    \"\"\")\n",
        "    print(\"âœ“ This will analyze all images with all three explainability methods\")\n",
        "    print(\"âœ“ Works with both directories and single image files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 2: Using Specific Explainability Methods\n",
        "\n",
        "Choose which methods to apply:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXAMPLE 2: Selective Method Usage\n",
            "================================================================================\n",
            "\n",
            "â„¹ No images found. Add an image to run this example.\n",
            "\n",
            "ðŸ“ USAGE EXAMPLE:\n",
            "\n",
            "explain_model_predictions(\n",
            "    model_path='microsoft/resnet-50',\n",
            "    data_path='./photo.jpg',                     # Single image file\n",
            "    output_dir='./outputs',\n",
            "    methods=['gradientshap', 'saliency']         # Only these methods\n",
            ")\n",
            "    \n",
            "âœ“ Works with single image files\n",
            "âœ“ Choose which explainability methods to apply\n",
            "âœ“ Available methods: 'gradientshap', 'integratedgradients', 'saliency'\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Analyze with Specific Methods\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, './src')\n",
        "\n",
        "from images import explain_model_predictions\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 2: Selective Method Usage\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Check for a single image file\n",
        "possible_files = []\n",
        "for ext in ['.jpg', '.jpeg', '.png']:\n",
        "    for path in ['./images/', './data/', './samples/', './']:\n",
        "        if os.path.exists(path):\n",
        "            files = [os.path.join(path, f) for f in os.listdir(path) \n",
        "                    if f.lower().endswith(ext) and os.path.isfile(os.path.join(path, f))]\n",
        "            if files:\n",
        "                possible_files.extend(files[:1])  # Take first match\n",
        "                break\n",
        "    if possible_files:\n",
        "        break\n",
        "\n",
        "if possible_files:\n",
        "    img_path = possible_files[0]\n",
        "    print(f\"\\nâœ“ Found image: {img_path}\")\n",
        "    print(f\"\\nAnalyzing with specific methods: GradientShap and Saliency\")\n",
        "    print(\"(Skipping Integrated Gradients for faster processing)\\n\")\n",
        "    \n",
        "    # Run with selected methods\n",
        "    explain_model_predictions(\n",
        "        model_path='microsoft/resnet-50',\n",
        "        data_path=img_path,                      # Single image file\n",
        "        output_dir='./method_comparison',\n",
        "        methods=['gradientshap', 'saliency']     # Only these methods\n",
        "    )\n",
        "    \n",
        "    print(\"\\nâœ“ Analysis complete! Check ./method_comparison/ for results\")\n",
        "else:\n",
        "    print(\"\\nâ„¹ No images found. Add an image to run this example.\")\n",
        "    print(\"\\n USAGE EXAMPLE:\")\n",
        "    print(\"\"\"\n",
        "explain_model_predictions(\n",
        "    model_path='microsoft/resnet-50',\n",
        "    data_path='./photo.jpg',                     # Single image file\n",
        "    output_dir='./outputs',\n",
        "    methods=['gradientshap', 'saliency']         # Only these methods\n",
        ")\n",
        "    \"\"\")\n",
        "    print(\"âœ“ Works with single image files\")\n",
        "    print(\"âœ“ Choose which explainability methods to apply\")\n",
        "    print(\"âœ“ Available methods: 'gradientshap', 'integratedgradients', 'saliency'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 3: Using Individual Functions (Advanced)\n",
        "\n",
        "Build custom workflows with modular functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXAMPLE 3: Custom Modular Workflow\n",
            "================================================================================\n",
            "\n",
            "[Step 1] Loading model...\n",
            "Using device: cpu\n",
            "Loading model: microsoft/resnet-50\n",
            "âœ“ Model loaded on cpu\n",
            "\n",
            "[Step 2] Getting image files...\n",
            "Directory detected: ./data/\n",
            "â„¹ No images found in common directories (./images/, ./data/, ./samples/)\n",
            "  To run this example, add images to one of these directories\n",
            "  For now, showing the workflow structure:\n",
            "\n",
            "  image_files = get_image_files('./images/', num_samples=10)\n",
            "  transform = create_transform(processor)\n",
            "âœ“ Model loaded on cpu\n",
            "\n",
            "[Step 2] Getting image files...\n",
            "Directory detected: ./data/\n",
            "â„¹ No images found in common directories (./images/, ./data/, ./samples/)\n",
            "  To run this example, add images to one of these directories\n",
            "  For now, showing the workflow structure:\n",
            "\n",
            "  image_files = get_image_files('./images/', num_samples=10)\n",
            "  transform = create_transform(processor)\n"
          ]
        }
      ],
      "source": [
        "# Example 3: Advanced - Build custom workflows with modular functions\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, './src')\n",
        "\n",
        "from helpers.image_utils import (\n",
        "    load_model,\n",
        "    get_image_files,\n",
        "    create_transform,\n",
        "    load_and_preprocess_image,\n",
        "    get_model_prediction\n",
        ")\n",
        "from images import (\n",
        "    compute_gradient_shap,\n",
        "    compute_saliency,\n",
        "    save_attribution_visualization\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 3: Custom Modular Workflow\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Step 1: Load model once (reuse for multiple images)\n",
        "print(\"\\n[Step 1] Loading model...\")\n",
        "model, processor, device = load_model('microsoft/resnet-50')\n",
        "print(f\"âœ“ Model loaded on {device}\")\n",
        "\n",
        "# Step 2: Get images (you can change this path to your images)\n",
        "print(\"\\n[Step 2] Getting image files...\")\n",
        "# For demo, we'll check if there are any images in common directories\n",
        "possible_paths = ['./images/', './data/', './samples/']\n",
        "image_files = []\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        image_files = get_image_files(path, num_samples=3)\n",
        "        if image_files:\n",
        "            print(f\"âœ“ Found {len(image_files)} images in {path}\")\n",
        "            break\n",
        "\n",
        "if not image_files:\n",
        "    print(\"â„¹ No images found in common directories (./images/, ./data/, ./samples/)\")\n",
        "    print(\"  To run this example, add images to one of these directories\")\n",
        "    print(\"  For now, showing the workflow structure:\")\n",
        "    print(\"\\n  image_files = get_image_files('./images/', num_samples=10)\")\n",
        "    print(\"  transform = create_transform(processor)\")\n",
        "else:\n",
        "    # Step 3: Create transform once\n",
        "    print(\"\\n[Step 3] Creating preprocessing transform...\")\n",
        "    transform = create_transform(processor)\n",
        "    print(\"âœ“ Transform created\")\n",
        "\n",
        "    # Step 4: Process each image with custom logic\n",
        "    print(\"\\n[Step 4] Processing images with custom logic...\")\n",
        "    print(\"  Strategy: Use expensive GradientShap only for high-confidence predictions\")\n",
        "    print(\"            Use faster Saliency for low-confidence predictions\\n\")\n",
        "    \n",
        "    os.makedirs('./custom_outputs', exist_ok=True)\n",
        "    \n",
        "    for idx, img_path in enumerate(image_files):\n",
        "        img_name = os.path.basename(img_path)\n",
        "        print(f\"\\n  Image {idx+1}/{len(image_files)}: {img_name}\")\n",
        "        \n",
        "        # Load and preprocess\n",
        "        original_img, input_tensor = load_and_preprocess_image(img_path, transform, device)\n",
        "        \n",
        "        # Get prediction\n",
        "        pred_class, pred_prob = get_model_prediction(model, input_tensor)\n",
        "        print(f\"    Prediction: Class {pred_class} (confidence: {pred_prob:.2%})\")\n",
        "        \n",
        "        # Custom logic: Choose method based on confidence\n",
        "        if pred_prob > 0.8:\n",
        "            print(f\"    High confidence â†’ Using GradientShap (more accurate but slower)\")\n",
        "            result = compute_gradient_shap(model, input_tensor, pred_class, device, n_samples=30)\n",
        "            method_name = \"GradientShap\"\n",
        "        else:\n",
        "            print(f\"    Lower confidence â†’ Using Saliency (faster)\")\n",
        "            result = compute_saliency(model, input_tensor, pred_class, device)\n",
        "            method_name = \"Saliency\"\n",
        "        \n",
        "        # Save result\n",
        "        if result:\n",
        "            output_path = f'./custom_outputs/{os.path.splitext(img_name)[0]}_{method_name.lower()}.png'\n",
        "            save_attribution_visualization(\n",
        "                original_img, \n",
        "                result['attributions'], \n",
        "                method_name, \n",
        "                output_path, \n",
        "                pred_class, \n",
        "                pred_prob\n",
        "            )\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"âœ“ Custom workflow completed!\")\n",
        "    print(f\"âœ“ Results saved to: ./custom_outputs/\")\n",
        "    print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 4: Demonstrate compute_integrated_gradients()\n",
        "\n",
        "Use Integrated Gradients method directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "COMMAND LINE INTERFACE (CLI) USAGE\n",
            "================================================================================\n",
            "\n",
            "Note: These commands are run in the terminal, not in this notebook\n",
            "\n",
            "\n",
            "# Example 1: Analyze all images in directory with all methods\n",
            "python src/images.py -data ./my_images/\n",
            "\n",
            "# Example 2: Single image with specific method\n",
            "python src/images.py -data ./photo.jpg -methods gradientshap\n",
            "\n",
            "# Example 3: Limit number of images, specific methods\n",
            "python src/images.py -data ./dataset/ -num_samples 5 -methods saliency integratedgradients\n",
            "\n",
            "# Example 4: Different model\n",
            "python src/images.py -data ./images/ -model google/vit-base-patch16-224\n",
            "\n",
            "# Example 5: Custom output directory\n",
            "python src/images.py -data ./images/ -outdir ./my_results/\n",
            "\n",
            "\n",
            "âœ“ Flexible command-line interface\n",
            "âœ“ Works with single files or directories\n",
            "âœ“ Choose methods and limit samples as needed\n",
            "\n",
            "â„¹ Note: CLI functionality requires removing __main__ block\n",
            "  (Already done - images.py is now a pure module)\n"
          ]
        }
      ],
      "source": [
        "# CLI Usage Examples (For Reference - Run these in terminal)\n",
        "print(\"=\" * 80)\n",
        "print(\"COMMAND LINE INTERFACE (CLI) USAGE\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nNote: These commands are run in the terminal, not in this notebook\\n\")\n",
        "\n",
        "examples = \"\"\"\n",
        "# Example 1: Analyze all images in directory with all methods\n",
        "python src/images.py -data ./my_images/\n",
        "\n",
        "# Example 2: Single image with specific method\n",
        "python src/images.py -data ./photo.jpg -methods gradientshap\n",
        "\n",
        "# Example 3: Limit number of images, specific methods\n",
        "python src/images.py -data ./dataset/ -num_samples 5 -methods saliency integratedgradients\n",
        "\n",
        "# Example 4: Different model\n",
        "python src/images.py -data ./images/ -model google/vit-base-patch16-224\n",
        "\n",
        "# Example 5: Custom output directory\n",
        "python src/images.py -data ./images/ -outdir ./my_results/\n",
        "\"\"\"\n",
        "\n",
        "print(examples)\n",
        "print(\"\\nâœ“ Flexible command-line interface\")\n",
        "print(\"âœ“ Works with single files or directories\")\n",
        "print(\"âœ“ Choose methods and limit samples as needed\")\n",
        "print(\"\\nâ„¹ Note: CLI functionality requires removing __main__ block\")\n",
        "print(\"  (Already done - images.py is now a pure module)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "KEY FEATURES OF THE MODULAR DESIGN\n",
            "================================================================================\n",
            "\n",
            "MODULARITY\n",
            "   - Each function has a single responsibility\n",
            "   - Functions can be used independently or combined\n",
            "   - Easy to test and maintain\n",
            "\n",
            "FLEXIBILITY\n",
            "   - Single image OR directory of images\n",
            "   - Choose specific explainability methods\n",
            "   - Custom workflows with individual functions\n",
            "   - Limit number of images processed\n",
            "\n",
            "REUSABILITY\n",
            "   - Load model once, reuse for multiple images\n",
            "   - Create preprocessing transform once\n",
            "   - Mix and match functions as needed\n",
            "\n",
            "EASE OF USE\n",
            "   - High-level function for quick analysis\n",
            "   - CLI for command-line usage  \n",
            "   - Python API for programmatic access\n",
            "   - Clear function signatures and returns\n",
            "\n",
            "EXPLAINABILITY METHODS\n",
            "   - GradientShap: Gradient-based Shapley values\n",
            "   - Integrated Gradients: Path integration method\n",
            "   - Saliency: Gradient magnitude highlighting\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Key Features Summary\n",
        "print(\"=\" * 80)\n",
        "print(\"KEY FEATURES OF THE MODULAR DESIGN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "features = \"\"\"\n",
        "MODULARITY\n",
        "   - Each function has a single responsibility\n",
        "   - Functions can be used independently or combined\n",
        "   - Easy to test and maintain\n",
        "\n",
        "FLEXIBILITY\n",
        "   - Single image OR directory of images\n",
        "   - Choose specific explainability methods\n",
        "   - Custom workflows with individual functions\n",
        "   - Limit number of images processed\n",
        "\n",
        "REUSABILITY\n",
        "   - Load model once, reuse for multiple images\n",
        "   - Create preprocessing transform once\n",
        "   - Mix and match functions as needed\n",
        "\n",
        "EASE OF USE\n",
        "   - High-level function for quick analysis\n",
        "   - CLI for command-line usage  \n",
        "   - Python API for programmatic access\n",
        "   - Clear function signatures and returns\n",
        "\n",
        "EXPLAINABILITY METHODS\n",
        "   - GradientShap: Gradient-based Shapley values\n",
        "   - Integrated Gradients: Path integration method\n",
        "   - Saliency: Gradient magnitude highlighting\n",
        "\"\"\"\n",
        "\n",
        "print(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "FUNCTION RETURN VALUES\n",
            "================================================================================\n",
            "\n",
            "load_model(model_path)\n",
            "â””â”€ Returns: (model, processor, device)\n",
            "\n",
            "compute_gradient_shap(...)\n",
            "compute_integrated_gradients(...)\n",
            "compute_saliency(...)\n",
            "â””â”€ Returns: dict or None\n",
            "    {\n",
            "        'attributions': numpy.ndarray,  # 2D attribution heatmap\n",
            "        'min': float,                    # Minimum attribution value\n",
            "        'max': float,                    # Maximum attribution value\n",
            "        'mean': float,                   # Mean absolute attribution\n",
            "        'method': str                    # Method name\n",
            "    }\n",
            "\n",
            "get_image_files(data_path, num_samples=None)\n",
            "â””â”€ Returns: list[str]  # List of image file paths\n",
            "\n",
            "create_transform(processor)\n",
            "â””â”€ Returns: torchvision.transforms.Compose\n",
            "\n",
            "load_and_preprocess_image(image_path, transform, device)\n",
            "â””â”€ Returns: (PIL.Image, torch.Tensor)\n",
            "\n",
            "get_model_prediction(model, input_tensor)\n",
            "â””â”€ Returns: (pred_class: int, pred_prob: float)\n",
            "\n",
            "\n",
            "âœ“ All functions have clear, documented return types\n",
            "âœ“ Attribution methods return None on error\n"
          ]
        }
      ],
      "source": [
        "# Function Return Values\n",
        "print(\"=\" * 80)\n",
        "print(\"FUNCTION RETURN VALUES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "returns = \"\"\"\n",
        "load_model(model_path)\n",
        "â””â”€ Returns: (model, processor, device)\n",
        "\n",
        "compute_gradient_shap(...)\n",
        "compute_integrated_gradients(...)\n",
        "compute_saliency(...)\n",
        "â””â”€ Returns: dict or None\n",
        "    {\n",
        "        'attributions': numpy.ndarray,  # 2D attribution heatmap\n",
        "        'min': float,                    # Minimum attribution value\n",
        "        'max': float,                    # Maximum attribution value\n",
        "        'mean': float,                   # Mean absolute attribution\n",
        "        'method': str                    # Method name\n",
        "    }\n",
        "\n",
        "get_image_files(data_path, num_samples=None)\n",
        "â””â”€ Returns: list[str]  # List of image file paths\n",
        "\n",
        "create_transform(processor)\n",
        "â””â”€ Returns: torchvision.transforms.Compose\n",
        "\n",
        "load_and_preprocess_image(image_path, transform, device)\n",
        "â””â”€ Returns: (PIL.Image, torch.Tensor)\n",
        "\n",
        "get_model_prediction(model, input_tensor)\n",
        "â””â”€ Returns: (pred_class: int, pred_prob: float)\n",
        "\"\"\"\n",
        "\n",
        "print(returns)\n",
        "print(\"\\nâœ“ All functions have clear, documented return types\")\n",
        "print(\"âœ“ Attribution methods return None on error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ERROR HANDLING PATTERNS\n",
            "================================================================================\n",
            "\n",
            "1. Safe Attribution Computation:\n",
            "\n",
            "   result = compute_gradient_shap(model, input_tensor, pred_class, device)\n",
            "   if result is None:\n",
            "       print(\"Attribution failed - check input tensor or model compatibility\")\n",
            "   else:\n",
            "       attributions = result['attributions']\n",
            "       print(f\"Attribution stats: min={result['min']:.3f}, max={result['max']:.3f}\")\n",
            "\n",
            "\n",
            "2. Robust Batch Processing:\n",
            "\n",
            "   successful = 0\n",
            "   failed = 0\n",
            "\n",
            "   for img_path in image_files:\n",
            "       try:\n",
            "           orig_img, input_t = load_and_preprocess_image(img_path, transform, device)\n",
            "           pred_c, pred_p = get_model_prediction(model, input_t)\n",
            "           result = compute_saliency(model, input_t, pred_c, device)\n",
            "\n",
            "           if result:\n",
            "               successful += 1\n",
            "           else:\n",
            "               failed += 1\n",
            "               print(f\"Attribution failed for {img_path}\")\n",
            "       except Exception as e:\n",
            "           failed += 1\n",
            "           print(f\"Error processing {img_path}: {e}\")\n",
            "\n",
            "   print(f\"Results: {successful} successful, {failed} failed\")\n",
            "\n",
            "\n",
            "PERFORMANCE OPTIMIZATION\n",
            "================================================================================\n",
            "\n",
            "1. Reuse Model (avoids repeated loading):\n",
            "\n",
            "   model, processor, device = load_model(model_path)  # Load once\n",
            "   transform = create_transform(processor)\n",
            "\n",
            "   for img in images:\n",
            "       # Reuse model, processor, transform for all images\n",
            "       ...\n",
            "\n",
            "\n",
            "2. Limit Samples for Testing:\n",
            "\n",
            "   # Test on small subset first\n",
            "   image_files = get_image_files('./data', num_samples=5)\n",
            "\n",
            "\n",
            "3. Choose Faster Methods:\n",
            "\n",
            "   # Saliency is fastest\n",
            "   explain_model_predictions(model_path, data_path, methods=['saliency'])\n",
            "\n",
            "   # GradientSHAP and Integrated Gradients are slower but more accurate\n",
            "   explain_model_predictions(model_path, data_path, methods=['gradientshap'])\n",
            "\n",
            "\n",
            "âœ“ All compute_* functions return None on error\n",
            "âœ“ Always check return values before using attributions\n",
            "âœ“ Load model once, reuse for all images\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ================================================================================\n",
        "# ADVANCED USAGE: ERROR HANDLING & PERFORMANCE\n",
        "# ================================================================================\n",
        "\n",
        "advanced_usage = \"\"\"\n",
        "ERROR HANDLING PATTERNS\n",
        "================================================================================\n",
        "\n",
        "1. Safe Attribution Computation:\n",
        "   \n",
        "   result = compute_gradient_shap(model, input_tensor, pred_class, device)\n",
        "   if result is None:\n",
        "       print(\"Attribution failed - check input tensor or model compatibility\")\n",
        "   else:\n",
        "       attributions = result['attributions']\n",
        "       print(f\"Attribution stats: min={result['min']:.3f}, max={result['max']:.3f}\")\n",
        "\n",
        "\n",
        "2. Robust Batch Processing:\n",
        "   \n",
        "   successful = 0\n",
        "   failed = 0\n",
        "   \n",
        "   for img_path in image_files:\n",
        "       try:\n",
        "           orig_img, input_t = load_and_preprocess_image(img_path, transform, device)\n",
        "           pred_c, pred_p = get_model_prediction(model, input_t)\n",
        "           result = compute_saliency(model, input_t, pred_c, device)\n",
        "           \n",
        "           if result:\n",
        "               successful += 1\n",
        "           else:\n",
        "               failed += 1\n",
        "               print(f\"Attribution failed for {img_path}\")\n",
        "       except Exception as e:\n",
        "           failed += 1\n",
        "           print(f\"Error processing {img_path}: {e}\")\n",
        "   \n",
        "   print(f\"Results: {successful} successful, {failed} failed\")\n",
        "\n",
        "\n",
        "PERFORMANCE OPTIMIZATION\n",
        "================================================================================\n",
        "\n",
        "1. Reuse Model (avoids repeated loading):\n",
        "   \n",
        "   model, processor, device = load_model(model_path)  # Load once\n",
        "   transform = create_transform(processor)\n",
        "   \n",
        "   for img in images:\n",
        "       # Reuse model, processor, transform for all images\n",
        "       ...\n",
        "\n",
        "\n",
        "2. Limit Samples for Testing:\n",
        "   \n",
        "   # Test on small subset first\n",
        "   image_files = get_image_files('./data', num_samples=5)\n",
        "\n",
        "\n",
        "3. Choose Faster Methods:\n",
        "   \n",
        "   # Saliency is fastest\n",
        "   explain_model_predictions(model_path, data_path, methods=['saliency'])\n",
        "   \n",
        "   # GradientSHAP and Integrated Gradients are slower but more accurate\n",
        "   explain_model_predictions(model_path, data_path, methods=['gradientshap'])\n",
        "\n",
        "\n",
        "âœ“ All compute_* functions return None on error\n",
        "âœ“ Always check return values before using attributions\n",
        "âœ“ Load model once, reuse for all images\n",
        "\"\"\"\n",
        "\n",
        "print(advanced_usage)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 4: Batch Processing with Custom Logic\n",
        "\n",
        "Process multiple images with your own logic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "COMMON ISSUES & SOLUTIONS\n",
            "================================================================================\n",
            "\n",
            "Issue: \"No images found in directory\"\n",
            "Solution: \n",
            "  â€¢ Check path exists and contains .jpg, .jpeg, .png files\n",
            "  â€¢ Use absolute paths or verify working directory\n",
            "  â€¢ Test with: get_image_files('./path/to/images')\n",
            "\n",
            "\n",
            "Issue: Attribution computation returns None\n",
            "Solution:\n",
            "  â€¢ Verify model and input tensor are on same device\n",
            "  â€¢ Check input tensor shape matches model requirements\n",
            "  â€¢ Ensure pred_class is valid integer index\n",
            "\n",
            "\n",
            "Issue: Memory errors with large batches\n",
            "Solution:\n",
            "  â€¢ Reduce num_samples or process in smaller batches\n",
            "  â€¢ Use saliency method (faster, less memory)\n",
            "  â€¢ Close matplotlib figures: plt.close('all')\n",
            "\n",
            "\n",
            "BEST PRACTICES\n",
            "================================================================================\n",
            "\n",
            "âœ“ Always load model once and reuse\n",
            "âœ“ Check return values (compute_* functions return None on error)\n",
            "âœ“ Use absolute paths for reliability\n",
            "âœ“ Start with small num_samples for testing\n",
            "âœ“ Choose methods based on speed/accuracy tradeoff:\n",
            "  - Saliency: Fastest, less accurate\n",
            "  - Integrated Gradients: Balanced\n",
            "  - GradientSHAP: Most accurate, slowest\n",
            "\n",
            "âœ“ For production: Add try/except blocks around image processing\n",
            "âœ“ For research: Use all methods and compare results\n",
            "\n",
            "\n",
            "QUICK VALIDATION\n",
            "================================================================================\n",
            "\n",
            "# Test setup without full analysis:\n",
            "from src.image import get_image_files, load_model\n",
            "\n",
            "# Verify images found\n",
            "images = get_image_files('./data', num_samples=1)\n",
            "print(f\"Found {len(images)} images\")\n",
            "\n",
            "# Verify model loads\n",
            "model, processor, device = load_model('microsoft/resnet-50')\n",
            "print(f\"Model loaded on {device}\")\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# ================================================================================\n",
        "# TROUBLESHOOTING & BEST PRACTICES\n",
        "# ================================================================================\n",
        "\n",
        "troubleshooting = \"\"\"\n",
        "COMMON ISSUES & SOLUTIONS\n",
        "================================================================================\n",
        "\n",
        "Issue: \"No images found in directory\"\n",
        "Solution: \n",
        "  â€¢ Check path exists and contains .jpg, .jpeg, .png files\n",
        "  â€¢ Use absolute paths or verify working directory\n",
        "  â€¢ Test with: get_image_files('./path/to/images')\n",
        "\n",
        "\n",
        "Issue: Attribution computation returns None\n",
        "Solution:\n",
        "  â€¢ Verify model and input tensor are on same device\n",
        "  â€¢ Check input tensor shape matches model requirements\n",
        "  â€¢ Ensure pred_class is valid integer index\n",
        "\n",
        "\n",
        "Issue: Memory errors with large batches\n",
        "Solution:\n",
        "  â€¢ Reduce num_samples or process in smaller batches\n",
        "  â€¢ Use saliency method (faster, less memory)\n",
        "  â€¢ Close matplotlib figures: plt.close('all')\n",
        "\n",
        "\n",
        "BEST PRACTICES\n",
        "================================================================================\n",
        "\n",
        "âœ“ Always load model once and reuse\n",
        "âœ“ Check return values (compute_* functions return None on error)\n",
        "âœ“ Use absolute paths for reliability\n",
        "âœ“ Start with small num_samples for testing\n",
        "âœ“ Choose methods based on speed/accuracy tradeoff:\n",
        "  - Saliency: Fastest, less accurate\n",
        "  - Integrated Gradients: Balanced\n",
        "  - GradientSHAP: Most accurate, slowest\n",
        "\n",
        "âœ“ For production: Add try/except blocks around image processing\n",
        "âœ“ For research: Use all methods and compare results\n",
        "\n",
        "\n",
        "QUICK VALIDATION\n",
        "================================================================================\n",
        "\n",
        "# Test setup without full analysis:\n",
        "from src.image import get_image_files, load_model\n",
        "\n",
        "# Verify images found\n",
        "images = get_image_files('./data', num_samples=1)\n",
        "print(f\"Found {len(images)} images\")\n",
        "\n",
        "# Verify model loads\n",
        "model, processor, device = load_model('microsoft/resnet-50')\n",
        "print(f\"Model loaded on {device}\")\n",
        "\"\"\"\n",
        "\n",
        "print(troubleshooting)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "1. **High-level usage** - `explain_model_predictions()` for complete automated analysis\n",
        "2. **Method selection** - Choose specific explainability methods\n",
        "3. **Modular approach** - Use individual functions for custom workflows\n",
        "4. **Integrated Gradients** - Direct use of `compute_integrated_gradients()`\n",
        "5. **Method comparison** - Side-by-side comparison with `save_comparison_visualization()`\n",
        "\n",
        "### All 6 Functions from images.py Demonstrated:\n",
        "âœ… `compute_gradient_shap()` - Example 3 & 5\n",
        "âœ… `compute_integrated_gradients()` - Example 4 & 5  \n",
        "âœ… `compute_saliency()` - Example 3 & 5\n",
        "âœ… `save_attribution_visualization()` - Examples 3 & 4\n",
        "âœ… `save_comparison_visualization()` - Example 5\n",
        "âœ… `explain_model_predictions()` - Examples 1 & 2\n",
        "\n",
        "### Key Takeaways:\n",
        "- Use `explain_model_predictions()` for quick, complete analysis\n",
        "- Use individual `compute_*` functions for custom workflows\n",
        "- Load model once with `load_model()`, reuse for all images\n",
        "- Handle both single images and directories with `get_image_files()`\n",
        "- Compare methods with `save_comparison_visualization()` to understand differences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Available Functions Reference\n",
        "\n",
        "### Model & Data Utilities (helpers/image_utils.py)\n",
        "- `ModelWrapper` - Wrapper class for HuggingFace models\n",
        "- `load_model(model_path)` - Load and prepare image classification model\n",
        "- `get_image_files(data_path, num_samples)` - Get image files from directory or single file\n",
        "- `create_transform(processor)` - Create preprocessing transformation pipeline\n",
        "- `load_and_preprocess_image(image_path, transform, device)` - Load and preprocess single image\n",
        "- `get_model_prediction(model, input_tensor)` - Get prediction class and confidence\n",
        "\n",
        "### Explainability Functions (images.py)\n",
        "1. **`compute_gradient_shap()`** - Compute GradientShap attributions (gradient-based Shapley values)\n",
        "2. **`compute_integrated_gradients()`** - Compute Integrated Gradients attributions (path integration)\n",
        "3. **`compute_saliency()`** - Compute Saliency attributions (gradient magnitude)\n",
        "4. **`save_attribution_visualization()`** - Save single attribution heatmap overlay\n",
        "5. **`save_comparison_visualization()`** - Save side-by-side comparison of all methods\n",
        "6. **`explain_model_predictions()`** - High-level orchestrator for complete analysis\n",
        "\n",
        "### Typical Workflow\n",
        "1. Load model â†’ `load_model('microsoft/resnet-50')`\n",
        "2. Get images â†’ `get_image_files('./data/', num_samples=10)`\n",
        "3. Create transform â†’ `create_transform(processor)`\n",
        "4. For each image:\n",
        "   - Load & preprocess â†’ `load_and_preprocess_image()`\n",
        "   - Get prediction â†’ `get_model_prediction()`\n",
        "   - Compute attributions â†’ `compute_gradient_shap()` / `compute_integrated_gradients()` / `compute_saliency()`\n",
        "   - Visualize â†’ `save_attribution_visualization()` or `save_comparison_visualization()`\n",
        "\n",
        "Or use `explain_model_predictions()` to do all steps automatically.\n",
        "\n",
        "### Method Selection Guide\n",
        "- **GradientShap**: Most accurate, considers baselines, slower (~2-5s per image)\n",
        "- **Integrated Gradients**: Balanced accuracy/speed, smooth attributions (~1-3s per image)\n",
        "- **Saliency**: Fastest, simple gradient-based, may be noisy (~0.5s per image)\n",
        "\n",
        "Use all three for research; use Saliency for quick testing; use GradientShap for production."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Concepts and Interpretations\n",
        "\n",
        "### Understanding Attribution Maps\n",
        "- **Hot colors (red/yellow)**: Regions that increase predicted class probability\n",
        "- **Cool colors (blue/purple)**: Regions that decrease predicted class probability  \n",
        "- **Intensity**: Strength of the pixel's contribution to the prediction\n",
        "- **White/transparent**: Pixels with minimal impact on prediction\n",
        "\n",
        "### Reading Visualizations\n",
        "- **Heatmap overlay**: Attribution values superimposed on original image\n",
        "- **Bright regions**: Most important for model's decision\n",
        "- **Attribution range**: Normalized to [0, 1] for visualization\n",
        "- **Mean absolute attribution**: Average importance across all pixels\n",
        "\n",
        "### Explainability Method Differences\n",
        "\n",
        "#### GradientShap\n",
        "- **Approach**: Combines gradients with Shapley value sampling\n",
        "- **Baseline**: Uses multiple baseline images (noise) for comparison\n",
        "- **Best for**: Identifying precise regions that distinguish the predicted class\n",
        "- **Interpretation**: \"This pixel makes the prediction more/less like class X compared to random noise\"\n",
        "\n",
        "#### Integrated Gradients\n",
        "- **Approach**: Integrates gradients along path from baseline to input\n",
        "- **Baseline**: Uses single baseline (typically black image or blurred version)\n",
        "- **Best for**: Smooth, coherent attribution maps with theoretical guarantees\n",
        "- **Interpretation**: \"This pixel's contribution accumulated from baseline to actual image\"\n",
        "\n",
        "#### Saliency\n",
        "- **Approach**: Simple gradient magnitude at input\n",
        "- **Baseline**: No baseline needed (single forward-backward pass)\n",
        "- **Best for**: Quick identification of high-gradient regions\n",
        "- **Interpretation**: \"This pixel has high sensitivity to output changes\"\n",
        "\n",
        "### Image Classification Specifics\n",
        "- **Edge detection**: Models often focus on edges and boundaries\n",
        "- **Texture patterns**: Repeated patterns may show consistent attributions\n",
        "- **Object parts**: Discriminative parts (eyes, wheels, etc.) typically highlighted\n",
        "- **Background**: Low attributions indicate irrelevant regions\n",
        "- **Context**: Sometimes surprising regions matter (e.g., typical backgrounds for certain objects)\n",
        "\n",
        "### Model Confidence vs. Attribution\n",
        "- **High confidence + focused attribution**: Model clearly identified key features\n",
        "- **High confidence + diffuse attribution**: Model uses many subtle cues\n",
        "- **Low confidence + scattered attribution**: Model uncertain, considering multiple possibilities\n",
        "- **Low confidence + focused attribution**: Model sees contradictory evidence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "\n",
        "1. **Test Different Models**: Compare ResNet, ViT, EfficientNet explanations\n",
        "2. **Analyze Failure Cases**: Examine misclassifications with low confidence\n",
        "3. **Compare Methods**: Use comparison visualizations to understand method differences\n",
        "4. **Domain-Specific Analysis**: Apply to medical images, materials science, etc.\n",
        "5. **Batch Processing**: Process large datasets with custom filtering logic\n",
        "6. **Attribution Statistics**: Analyze attribution patterns across image categories\n",
        "7. **Model Debugging**: Use attributions to identify model biases or spurious correlations\n",
        "8. **Fine-tuning Validation**: Verify fine-tuned models focus on correct features\n",
        "\n",
        "### Advanced Applications\n",
        "- **Model Comparison**: Compare attributions from different architectures on same images\n",
        "- **Adversarial Analysis**: Study how attributions change under adversarial perturbations\n",
        "- **Feature Validation**: Confirm models use domain-relevant features (e.g., medical diagnostics)\n",
        "- **Trust Building**: Show stakeholders what the model \"sees\" for transparency"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXAMPLE 4: Integrated Gradients Method\n",
            "================================================================================\n",
            "Directory detected: ./data/fracture_samples\n",
            "\n",
            "âœ“ Found image in: ./data/fracture_samples\n",
            "\n",
            "[Step 1] Loading model...\n",
            "Using device: cpu\n",
            "Loading model: microsoft/resnet-50\n",
            "âœ“ Model loaded on cpu\n",
            "\n",
            "[Step 2] Processing: brick_texture.png\n",
            "âœ“ Prediction: Class 828 (confidence: 48.66%)\n",
            "\n",
            "[Step 3] Computing Integrated Gradients attributions...\n",
            "Computing Integrated Gradients attributions...\n",
            "IntegratedGradients attribution range: [0.0001, 0.2681]\n",
            "IntegratedGradients mean absolute attribution: 0.0154\n",
            "âœ“ Attribution range: [0.0001, 0.2681]\n",
            "âœ“ Mean absolute attribution: 0.0154\n",
            "    Saved: ./outputs/integrated_gradients/brick_texture_ig.png\n",
            "\n",
            "âœ“ Saved: ./outputs/integrated_gradients/brick_texture_ig.png\n",
            "\n",
            "Integrated Gradients provides smooth, theoretically-grounded attributions\n",
            "by integrating gradients along the path from baseline to input.\n"
          ]
        }
      ],
      "source": [
        "# Example 4: Using compute_integrated_gradients() directly\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, './src')\n",
        "\n",
        "from helpers.image_utils import (\n",
        "    load_model,\n",
        "    get_image_files,\n",
        "    create_transform,\n",
        "    load_and_preprocess_image,\n",
        "    get_model_prediction\n",
        ")\n",
        "from images import (\n",
        "    compute_integrated_gradients,\n",
        "    save_attribution_visualization\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 4: Integrated Gradients Method\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Find an image\n",
        "possible_paths = ['./data/fracture_samples', './data/generic_samples', './data/sample_images', './images/', './data/']\n",
        "image_files = []\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        image_files = get_image_files(path, num_samples=1)\n",
        "        if image_files:\n",
        "            print(f\"\\nâœ“ Found image in: {path}\")\n",
        "            break\n",
        "\n",
        "if image_files:\n",
        "    # Load model\n",
        "    print(\"\\n[Step 1] Loading model...\")\n",
        "    model, processor, device = load_model('microsoft/resnet-50')\n",
        "    print(f\"âœ“ Model loaded on {device}\")\n",
        "    \n",
        "    # Process image\n",
        "    img_path = image_files[0]\n",
        "    img_name = os.path.basename(img_path)\n",
        "    print(f\"\\n[Step 2] Processing: {img_name}\")\n",
        "    \n",
        "    transform = create_transform(processor)\n",
        "    original_img, input_tensor = load_and_preprocess_image(img_path, transform, device)\n",
        "    pred_class, pred_prob = get_model_prediction(model, input_tensor)\n",
        "    \n",
        "    print(f\"âœ“ Prediction: Class {pred_class} (confidence: {pred_prob:.2%})\")\n",
        "    \n",
        "    # Compute Integrated Gradients\n",
        "    print(\"\\n[Step 3] Computing Integrated Gradients attributions...\")\n",
        "    result = compute_integrated_gradients(\n",
        "        model, \n",
        "        input_tensor, \n",
        "        pred_class, \n",
        "        device, \n",
        "        n_steps=100  # More steps = smoother attributions\n",
        "    )\n",
        "    \n",
        "    if result:\n",
        "        print(f\"âœ“ Attribution range: [{result['min']:.4f}, {result['max']:.4f}]\")\n",
        "        print(f\"âœ“ Mean absolute attribution: {result['mean']:.4f}\")\n",
        "        \n",
        "        # Save visualization\n",
        "        os.makedirs('./outputs/integrated_gradients', exist_ok=True)\n",
        "        output_path = f\"./outputs/integrated_gradients/{os.path.splitext(img_name)[0]}_ig.png\"\n",
        "        \n",
        "        save_attribution_visualization(\n",
        "            original_img,\n",
        "            result['attributions'],\n",
        "            'Integrated Gradients',\n",
        "            output_path,\n",
        "            pred_class,\n",
        "            pred_prob\n",
        "        )\n",
        "        \n",
        "        print(f\"\\nâœ“ Saved: {output_path}\")\n",
        "        print(\"\\nIntegrated Gradients provides smooth, theoretically-grounded attributions\")\n",
        "        print(\"by integrating gradients along the path from baseline to input.\")\n",
        "    else:\n",
        "        print(\"âœ— Attribution computation failed\")\n",
        "else:\n",
        "    print(\"\\nâ„¹ No images found. Add images to run this example.\")\n",
        "    print(\"\\nðŸ“ CODE STRUCTURE:\")\n",
        "    print(\"\"\"\n",
        "# Load model once\n",
        "model, processor, device = load_model('microsoft/resnet-50')\n",
        "transform = create_transform(processor)\n",
        "\n",
        "# For each image\n",
        "original_img, input_tensor = load_and_preprocess_image(img_path, transform, device)\n",
        "pred_class, pred_prob = get_model_prediction(model, input_tensor)\n",
        "\n",
        "# Compute Integrated Gradients\n",
        "result = compute_integrated_gradients(model, input_tensor, pred_class, device, n_steps=100)\n",
        "\n",
        "# Save visualization\n",
        "save_attribution_visualization(original_img, result['attributions'], \n",
        "                              'Integrated Gradients', output_path, \n",
        "                              pred_class, pred_prob)\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Method 5: Demonstrate save_comparison_visualization()\n",
        "\n",
        "Compare all three methods side-by-side:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXAMPLE 5: Method Comparison Visualization\n",
            "================================================================================\n",
            "Directory detected: ./data/fracture_samples\n",
            "\n",
            "âœ“ Found image in: ./data/fracture_samples\n",
            "\n",
            "[Step 1] Loading model...\n",
            "Using device: cpu\n",
            "Loading model: microsoft/resnet-50\n",
            "âœ“ Model loaded on cpu\n",
            "\n",
            "[Step 2] Processing: brick_texture.png\n",
            "âœ“ Prediction: Class 828 (confidence: 48.66%)\n",
            "\n",
            "[Step 3] Computing all three attribution methods...\n",
            "  â€¢ Computing GradientShap...\n",
            "Computing GradientShap attributions...\n",
            "GradientShap attribution range: [0.0001, 0.0927]\n",
            "GradientShap mean absolute attribution: 0.0058\n",
            "    âœ“ Range: [0.0001, 0.0927]\n",
            "  â€¢ Computing Integrated Gradients...\n",
            "Computing Integrated Gradients attributions...\n",
            "IntegratedGradients attribution range: [0.0001, 0.2719]\n",
            "IntegratedGradients mean absolute attribution: 0.0155\n",
            "    âœ“ Range: [0.0001, 0.2719]\n",
            "  â€¢ Computing Saliency...\n",
            "Computing Saliency attributions...\n",
            "Saliency attribution range: [0.0007, 1.1264]\n",
            "Saliency mean absolute attribution: 0.0588\n",
            "    âœ“ Range: [0.0007, 1.1264]\n",
            "\n",
            "[Step 4] Creating comparison visualization...\n",
            "Saved: ./outputs/comparisons/brick_texture_comparison.png\n",
            "âœ“ Saved comparison: ./outputs/comparisons/brick_texture_comparison.png\n",
            "\n",
            "================================================================================\n",
            "COMPARISON INSIGHTS:\n",
            "================================================================================\n",
            "â€¢ GradientShap: Most stable, uses baseline comparisons\n",
            "â€¢ Integrated Gradients: Smoothest, theoretically grounded\n",
            "â€¢ Saliency: Fastest, highlights high-gradient regions\n",
            "\n",
            "View the comparison image to see how each method highlights\n",
            "different aspects of the image for the same prediction!\n"
          ]
        }
      ],
      "source": [
        "# Example 5: Using save_comparison_visualization() to compare all methods\n",
        "import sys\n",
        "import os\n",
        "sys.path.insert(0, './src')\n",
        "\n",
        "from helpers.image_utils import (\n",
        "    load_model,\n",
        "    get_image_files,\n",
        "    create_transform,\n",
        "    load_and_preprocess_image,\n",
        "    get_model_prediction\n",
        ")\n",
        "from images import (\n",
        "    compute_gradient_shap,\n",
        "    compute_integrated_gradients,\n",
        "    compute_saliency,\n",
        "    save_comparison_visualization\n",
        ")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 5: Method Comparison Visualization\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Find an image\n",
        "possible_paths = ['./data/fracture_samples', './data/generic_samples', './data/sample_images', './images/', './data/']\n",
        "image_files = []\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        image_files = get_image_files(path, num_samples=1)\n",
        "        if image_files:\n",
        "            print(f\"\\nâœ“ Found image in: {path}\")\n",
        "            break\n",
        "\n",
        "if image_files:\n",
        "    # Load model\n",
        "    print(\"\\n[Step 1] Loading model...\")\n",
        "    model, processor, device = load_model('microsoft/resnet-50')\n",
        "    print(f\"âœ“ Model loaded on {device}\")\n",
        "    \n",
        "    # Process image\n",
        "    img_path = image_files[0]\n",
        "    img_name = os.path.basename(img_path)\n",
        "    print(f\"\\n[Step 2] Processing: {img_name}\")\n",
        "    \n",
        "    transform = create_transform(processor)\n",
        "    original_img, input_tensor = load_and_preprocess_image(img_path, transform, device)\n",
        "    pred_class, pred_prob = get_model_prediction(model, input_tensor)\n",
        "    \n",
        "    print(f\"âœ“ Prediction: Class {pred_class} (confidence: {pred_prob:.2%})\")\n",
        "    \n",
        "    # Compute all three attribution methods\n",
        "    print(\"\\n[Step 3] Computing all three attribution methods...\")\n",
        "    attributions_dict = {}\n",
        "    \n",
        "    print(\"  â€¢ Computing GradientShap...\")\n",
        "    result_gs = compute_gradient_shap(model, input_tensor, pred_class, device, n_samples=50)\n",
        "    if result_gs:\n",
        "        attributions_dict['GradientShap'] = result_gs['attributions']\n",
        "        print(f\"    âœ“ Range: [{result_gs['min']:.4f}, {result_gs['max']:.4f}]\")\n",
        "    \n",
        "    print(\"  â€¢ Computing Integrated Gradients...\")\n",
        "    result_ig = compute_integrated_gradients(model, input_tensor, pred_class, device, n_steps=50)\n",
        "    if result_ig:\n",
        "        attributions_dict['IntegratedGradients'] = result_ig['attributions']\n",
        "        print(f\"    âœ“ Range: [{result_ig['min']:.4f}, {result_ig['max']:.4f}]\")\n",
        "    \n",
        "    print(\"  â€¢ Computing Saliency...\")\n",
        "    result_sal = compute_saliency(model, input_tensor, pred_class, device)\n",
        "    if result_sal:\n",
        "        attributions_dict['Saliency'] = result_sal['attributions']\n",
        "        print(f\"    âœ“ Range: [{result_sal['min']:.4f}, {result_sal['max']:.4f}]\")\n",
        "    \n",
        "    # Create comparison visualization\n",
        "    if len(attributions_dict) == 3:\n",
        "        print(\"\\n[Step 4] Creating comparison visualization...\")\n",
        "        os.makedirs('./outputs/comparisons', exist_ok=True)\n",
        "        output_path = f\"./outputs/comparisons/{os.path.splitext(img_name)[0]}_comparison.png\"\n",
        "        \n",
        "        save_comparison_visualization(\n",
        "            original_img,\n",
        "            attributions_dict,\n",
        "            output_path,\n",
        "            pred_class,\n",
        "            pred_prob,\n",
        "            img_name\n",
        "        )\n",
        "        \n",
        "        print(f\"âœ“ Saved comparison: {output_path}\")\n",
        "        print(\"\\n\" + \"=\" * 80)\n",
        "        print(\"COMPARISON INSIGHTS:\")\n",
        "        print(\"=\" * 80)\n",
        "        print(\"â€¢ GradientShap: Most stable, uses baseline comparisons\")\n",
        "        print(\"â€¢ Integrated Gradients: Smoothest, theoretically grounded\")\n",
        "        print(\"â€¢ Saliency: Fastest, highlights high-gradient regions\")\n",
        "        print(\"\\nView the comparison image to see how each method highlights\")\n",
        "        print(\"different aspects of the image for the same prediction!\")\n",
        "    else:\n",
        "        print(f\"\\nâœ— Only {len(attributions_dict)}/3 methods succeeded\")\n",
        "else:\n",
        "    print(\"\\nâ„¹ No images found. Add images to run this example.\")\n",
        "    print(\"\\nðŸ“ CODE STRUCTURE:\")\n",
        "    print(\"\"\"\n",
        "# Compute all three methods\n",
        "attributions_dict = {}\n",
        "attributions_dict['GradientShap'] = compute_gradient_shap(...)['attributions']\n",
        "attributions_dict['IntegratedGradients'] = compute_integrated_gradients(...)['attributions']\n",
        "attributions_dict['Saliency'] = compute_saliency(...)['attributions']\n",
        "\n",
        "# Create side-by-side comparison\n",
        "save_comparison_visualization(\n",
        "    original_img,\n",
        "    attributions_dict,\n",
        "    output_path,\n",
        "    pred_class,\n",
        "    pred_prob,\n",
        "    image_name\n",
        ")\n",
        "    \"\"\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
